<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[跨语言的Verilator仿真：使用进程间通信]]></title>
    <url>%2F2019%2F07%2F25%2F%E8%B7%A8%E8%AF%AD%E8%A8%80%E7%9A%84Verilator%E4%BB%BF%E7%9C%9F%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%2F</url>
    <content type="text"><![CDATA[最近在研究Chisel和高层次的硬件开发框架的项目，发现做仿真这块的方法也是五花八门。有些项目可能自己做了一套仿真的框架，比如PyMTL就自己设计了一套仿真的方案，是直接使用Python来实现的，其阐述的目的主要就是为了解决Methodology Gap。最受关注的Chisel的仿真则是在框架的后端直接调用Verilator来进行仿真。不容置疑，目前来说开源仿真的工具中速度最快性能最好的就是Verilator了，虽然其编译的时间可能要稍长。PyMTL的仿真方案速度自然也比不上Verilator，然而其标榜点也并不是速度，也没什么可说的，加之其还使用了自定义的PyPy来进行仿真过程中的JIT加速，也算是一个有趣的亮点，不过这也是后话了。 Verilator的原理实际上是将Verilog源码编译成单/多线程的C++源代码来进行仿真，其将所需要仿真的dut编译为一个类，dut的IO口则被编译为类成员。Verilator不单单只是简单的把Verilog编译为C++，Verilator还会将代码进行优化，编译成优化过的C++模型来进行仿真。 Chisel的PeekPokeTester可以调用Verilator作为后端来进行仿真，其仿真的主要原理就是通过进程间的通信来实现的。在这篇文章中，我将会给出一个通过Python来进行跨语言仿真的例子。在此之前，我们首先要清楚Verilator仿真是如何使用的。 Verilator仿真——编写C++ harness前面已经说过，Verilator的仿真是将Verilog代码编译成等价的C++类模型，那么如何来操纵这个C++类模型来达到使用者所需要的仿真过程？首先，假设我们写了一个简单的译码器： 12345678910111213141516171819202122232425// Decoder.vmodule Decoder( input reset, input [2:0] S, output [7:0] out); always @* begin if(reset == 1'b1) out = 8'b00000000; else begin case(S) 3'b000: out = 8'b00000001; 3'b001: out = 8'b00000010; 3'b010: out = 8'b00000100; 3'b011: out = 8'b00001000; 3'b100: out = 8'b00010000; 3'b101: out = 8'b00100000; 3'b110: out = 8'b01000000; 3'b111: out = 8'b10000000; endcase end endendmodule 通过调用Verilator，可以编译为等价的C++类模型： 1234# --cc 后跟所需要编译的verilog源代码文件# --trace 添加生成.vcd波形文件的功能# 需要注意的是，模块名称必须要和文件名称一致$ verilator --cc Decoder.v --trace 执行后将会在当前目录生成一个obj_dir的文件夹，里面含有一堆编译得到的头文件和C++源文件，这个在我们书写C++ harness的时候有帮助。 C++ harnessC++ harness可以理解为你自己规定仿真过程，并使用Verilator规定的语义来进行表达。书写其有一定的规则，但是只做一些比较基本的仿真，所需要了解的并不多，如下： 包含Verilator核心头文件：verilated.h。如果要加上生成.vcd波形文件的支持，还需要包含verilated_vcd_c.h。同时将需要进行仿真的Verilog代码编译得到的头文件包含进来，如上述的译码器则需要包含头文件VDecoder.h。 初始化模块对象以及波性文件对象，如上例子分别对应VDecoder以及VerilatedVcdC对象。并对Verilator做适当的初始化工作。 编写仿真流程，其核心则是dut顶层对象的eval()方法。 清理工作，包括指针指向空间释放以及文件流关闭等工作。 下面是一个对上述译码器进行仿真的C++ harness的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// Decoder-harness.cpp#include &lt;verilated.h&gt; // 核心头文件#include &lt;verilated_vcd_c.h&gt; // 波形生成头文件#include &lt;iostream&gt;#include &lt;fstream&gt;#include "VDecoder.h" // 译码器模块类using namespace std;VDecoder* top; // 顶层dut对象指针VerilatedVcdC* tfp; // 波形生成对象指针vluint64_t main_time = 0; // 仿真时间戳const vluint64_t sim_time = 1024; // 最大仿真时间戳int main(int argc, char **argv)&#123; // 一些初始化工作 Verilated::commandArgs(argc, argv); Verilated::traceEverOn(true); // 为对象分配内存空间 top = new VDecoder; tfp = new VerilatedVcdC; // tfp初始化工作 top-&gt;trace(tfp, 99); tfp-&gt;open("Decoder.vcd"); int count = 0; while(!Verilated::gotFinish() &amp;&amp; main_time &lt; sim_time) &#123; // 仿真过程 top-&gt;reset = 0; top-&gt;S = count; // 模块S输出递增 top-&gt;eval(); // 仿真时间步进 tfp-&gt;dump(main_time); // 波形文件写入步进 count++; main_time++; &#125; // 清理工作 tfp-&gt;close(); delete top; delete tfp; exit(0); return 0;&#125; C++ harness写完后，执行指令并编译成一个可执行文件： 12$ verilator --cc Decoder.v --trace --exe Decoder-harness.cpp$ make -j -C ./obj_dir -f VDecoder.mk VDecoder 在obj_dir目录下生成一个VDecoder的可执行文件，执行后即可生成所需要的.vcd波形文件。 跨语言Verilator仿真通过上面的描述，实际上我们可以很自然的想到如何在别的语言中调用Verilator来做实时仿真的工作，这实际上就涉及到了使用进程间通信的问题。上面已经提到了，Verilator会将Verilog代码编译成特定的模块类，而模块的IO口就是模块类中的数据成员，在每一步的仿真步进当中，都可以获取模块类属性的值来得到当前模块IO的取值。在前端程序中（这里是Python）发送信号给Verilator仿真程序，指定其动作（输入、输出、开始、结束、步进等）来进行跨语言调用Verilator进行仿真。 C++做进程间通信的方法非常多，信号、管道、共享内存等等。但是要考虑一种Python与C++通用的方法，我最后是使用了两者都有的文件映射内存的功能mmap，来作为一个管道（pipe）。我一共使用了3个管道，分别是in.dat、out.dat以及sig.dat，分别对应仿真程序的输入、输出以及信号管道。 C++ mmapC++使用memory mapping功能需要包含库文件sys/mman.h（注意这个是Linux特有的）。创建mmap的函数文档如下： 1void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset); 参数： start: 映射区开始的地址。若为NULL，则会自动分配。 length: 内存段的大小。 prot: 设置内存段访问的权限。一般情况下选取PROT_READ | PROT_WRITE即可 PROT_READ:允许读该内存段 PROT_WRITE:允许写该内存段 PROT_EXEC:允许执行该内存段 PROT_NONE:不能访问 flags: 控制程序对内存段的改变所造成的影响。一般情况下使用MAP_SHARED MAP_PRIVATE: 内存段私有，对它的修改值对本进程有效 MAP_SHARED: 把对该内存段的修改保存到磁盘文件中 MAP_FIXED: 该内存段必须位于addr指定的地址处 fd: 打开的文件描述符 off: 用以改变经共享内存段访问的文件中数据的起始偏移值 创建mmap的方式很简单，调用上述函数即可。然而实际上这里有很多坑，首先length的大小要是系统内存分页大小的整数倍，这个可以通过getpagesize()来获取。还有就是打开的文件描述符也需要使用ftruncate()函数来将文件的大小截取为映射到内存的大小，否则就会Bus Error。一个完整的mmap初始化过程如下： 1234int fd = open("file.dat", O_RDWR | O_CREAT, 00777);int psize = getpagesize();ftruncate(fd, psize);int *mm = (int*)mmap(NULL, psize, PROT_WRITE | PROT_READ, MAP_SHARED, fd, 0); 接下来，你就可以将这个文件完全当做一个内存空间来看待，比如使用mm[0]、mm[1]等方法来访问这个内存空间中不同地址的数据（当然注意不要越界）。 最后是munmap()函数，它会关闭文件的映射，并将对应内存空间中的内容写回到文件当中： 1int munmap(void *start, size_t length); 其中start为内存映射的起始地址，length为内存映射的空间大小。 Python mmap相较之下，python使用mmap要简单很多，只需要导入mmap模块即可： 12psize = 4096mm = mmap.mmap(os.open("file.dat", os.O_RDWR), psize) 此时mm可以使用文件流的方法，比如使用write()来往文件中写入数据，需要注意的是数据必须是bytes类型的对象，使用read()来读取指定大小的数据（单位为字节）。 在映射完成后，调用close()方法即可： 1mm.close() 进程间通信进程间通信所使用的就是sig.dat这条管道，可以在双方规定信号的值。例如等待WAIT为0，输入到达DIN为1等等。同时双方还要协商好一个数据单元的大小（32位的int还是64位？）。在我给出的例子中，C++仿真程序端的信号同步例子如下： 1234567891011121314151617while(1)&#123; while (*sig == WAIT); // 轮询 if (*sig == TERM) break; switch (*sig) &#123; case DIN: input_value(in); break; // 从in.dat中获取数据 case DOUT: output_value(out); break; // 写数据到out.dat case STEP: step(); break; // 仿真步进 default: break; &#125; *sig = WAIT; // 等待下一个信号到来&#125; 在Python中，等待信号则为： 1234def wait_signal(): signal = read_data(mm_sig) while signal != WAIT: signal = read_data(mm_sig) 至此，信号同步的协定完成，就可以双方通过文件映射的共享内存空间进行通信了。 上面所使用的例子均在仓库 https://github.com/Groupsun/Verilator_Simple_Usage 中的/pipe文件夹中提供。可以参照ALU的例子： ALU.v: Verilog源码 Sim_ALU.cpp: C++ harness ALU_wrapper.py: python调用]]></content>
      <categories>
        <category>Computer-Aid Design</category>
      </categories>
      <tags>
        <tag>Verilator</tag>
        <tag>Simulation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ISA Design 101]]></title>
    <url>%2F2019%2F07%2F06%2FISA-Design-101%2F</url>
    <content type="text"><![CDATA[计算机架构师在设计ISA时应该遵循的原则以及利弊权衡。首先列举以下的7个衡量指标： 成本（cost） 简洁性（simplicity） 性能（performance） 架构与实现之间的分离（isolation of architecture from implementation） 提升空间（room for growth） 程序大小（program size） 易于编程/编译/链接（ease of programming/compiling/linking） 成本处理器被实现为集成电路的形式，一般称为芯片（chips）或者晶粒（dies）。之所以被称为晶粒，这是因为处理器生产从一块单一的圆形晶圆切割成许多独立的晶粒得到。因此，成本对于晶粒的面积是非常敏感的： $$ cost \approx f(die , area^2) $$ 显然，晶粒的大小越小，一块晶圆所能切割出的晶粒数量越多，并且大多数的成本来自于对晶圆本身的处理。一个不太准确的说法是，晶粒的大小越小，晶粒的产率越高。因为硅片生产商在处理晶粒的时候会在晶圆上留下一小部分的瑕疵，因此晶粒的大小越小，那么晶圆的利用率就越高。 一个指令集的架构师会设法使指令集架构足够简洁，来使得实现的处理器的大小足够小。 简洁性鉴于成本对于复杂度的敏感性，架构师需要一个简单的ISA来缩小芯片面积。一个足够简洁的指令集架构可以缩短芯片设计以及验证的时间，这些时间可以占据芯片生产过程的大部分成本。这些成本必须要加在芯片的总的生产成本中。简洁性还会影响文档的大小以及用户理解该ISA的难易程度。 性能除去一些面向嵌入式系统的迷你芯片以外，一般来说架构师还需要考虑性能的问题。性能可以被3个项相乘所得到： $$ \frac{instructions}{program} \times \frac{average clock cycles}{instruction} \times \frac{time}{clock cycle} = \frac{time}{program} $$ 也就是说，决定处理器性能的有三方面的因素：程序的平均指令数、每条指令执行的平均周期数以及每个时钟周期的长度。 架构以及实现之间的分离对架构以及实现之间的分离，可以最早追溯到上世纪60年代。当时架构指的是一个机器语言的程序员需要知道如何去写一个正确的程序，但是不能保证该程序的性能。对于一个架构师来说，一件颇具诱惑的事情是，为了提高某些特定时间中的特定功能实现的效率，而加入新的指令，然而这在会在未来的实现中带来不必要的困难。 举一个令人遗憾的例子来说，就是在MIPS-32当中所引入的延迟分支技术。当时MIPS的设计人员针对条件分支所带来的控制冒险问题，使用延迟分支的方法（有时也称为分支延迟间隙）来解决。它将条件分支的指令重新定义，将其定义为在条件分支指令下一条指令执行完后才发生，程序员或者编译器通过在这个周期的间隙中加入有用的指令来执行。 然而，这个所谓的解决方案对于之后流水线级数更深的MIPS处理器来说没有作用，而且还使得MIPS-32的程序员、编译器的编写人员以及处理器的设计者更难受，并且难以看懂MIPS-32的汇编代码。 这个例子告诉我们，架构师不能将一些只会对当前一个实现有帮助的特性加入到指令集当中，同时，架构师也不应该放入阻碍某些实现的功能。例如，ARM-32和其他一些ISA具有Load Multiple指令。这些指令可以提高单发射流水线设计的性能，但会降低多发射流水线的效率。原因在于这种直截了当的实现排除了与其他指令并行地调度Load Multiple的各个负载的可能，从而降低了这些处理器的指令吞吐量。 提升空间随着摩尔定律的放缓，对指令集的成本-性能提升的唯一出路是在其中增加一些专为特定领域所使用的定制指令，比如深度学习、强化现实、组合优化、图形学等等。这意味着在今天，ISA对于保留一定的操作码空间是非常重要的。 在上世纪的70、80年代，当时摩尔定律还在如日中天的时候，很少有架构师会有为操作码保留空间的想法，与此相反，架构师重视更长的地址以及立即数字段来减少每个程序平均的指令数，也就是之前所提到的三个因素中的第一个因素。 一个缺乏操作码空间所带来的冲击的例子就是，ARM-32与Thumb的例子。当时，ARM-32的架构师想尝试去降低代码大小，它们想在标准的32位长度的ISA中增加16位长度的指令，但是由于指令中已经没有空间进行压缩了，因此他们只能够创造一个新的16位ISA（Thumb），然后同时用 16 位指令和 32 位指令来组成另外一个ISA（Thumb-2），它们与ARM ISA之间可以通过一个模式位来进行切换。切换的方式是通过在地址的最低位设为1来实现，这是因为16位和32位的地址中这一位必然为0。 程序大小程序的大小越小，则在芯片上的程序存储器所需要的空间就越小，这在嵌入式设备中是一个显著的成本开销。因此，这个因素促使ARM的架构师后来在Thumb以及Thumb-2中增加更短的指令。更小的程序同时还会使得指令缓存的命中率更高，节省更多的功耗，因为访问片外的DRAM所需要功耗要比访问片内的SRAM更高。同样，这种做法也能提升性能。更小的代码大小是ISA架构师的重要目标之一。 x86是变长的指令集架构，x86-32具有短至1个字节的指令，长至15个字节的指令。大多数人可能会认为，使用可变长度指令的指令集所书写的程序，肯定比使用如32位定长指令集，如ARM-32和RISC-V所书写的程序大小要小。从逻辑上来说，8位的可变长指令集所写出来的程序同样比只提供16位和32位的定长指令集，如Thumb-2和RV32C，所写出来的程序要小。事实上，同一个程序，ARM-32以及RISC-V的代码大小要比x86-32要分别大6%以及9%，然而x86-32却要比压缩版本的要大26%（RV32C和Thumnb-2）。 一个具有8位可变长度的ISA确实要比使用RV32C和Thumb-2所写的程序代码长度要更小，但是在上世纪70年代，x86的架构师有着不同的想法。为了使指令日益增加的ISA达到向后二进制兼容性的要求，大量x86-32的指令比原来所期望的要更长，因此它们只能使用原x86中的操作码空间。 易于编程/编译/链接由于在寄存器中的数据访问速度远比访问内存中的数据快，因此对于编译器来说，好的寄存器分配策略更为关键。因此，如果指令集中提供的寄存器越多，那么这项工作就越简单。因此，大多数的RISC都拥有大量的指令集，RISC-V中有32个（RV32I），ARM-32中有16个，而x86-32中只有8个。更多的寄存器能够使编译器以及汇编程序员更为好受。 对于编译器和汇编程序员来说，另外一个问题是如何弄清楚一个代码序列执行的速度。RISC-V的指令典型的执行速度是1个指令最多1个时钟周期（忽略缓存未命中的情况）。然而ARM-32和x86-32有很多的指令需要很多个时钟周期才能执行完毕（甚至是在所有缓存命中的情况下）。况且，x86-32还具有直接在内存中进行操作的指令（不同于加载存储架构的RISC-V，对内存的操作只能是读取和写入内存）。在内存中复杂的指令和操作会使得处理器的设计者难以预测处理器的执行性能。 对于ISA来说，支持位置无关代码（position independent code，PIC）是有用的。因为这样就可以支持动态链接，原因在于共享的库代码可以驻留在不同程序的不同地址上。因此，与PC相关的分支和数据寻址对于PIC来说是一大福音。虽然几乎绝大多数的ISA都提供了PC相关的寻址模式，但是x86-32和MIPS-32并没有提供。]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>Computer Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[8 great ideas in computer architecture]]></title>
    <url>%2F2019%2F07%2F06%2F8-great-ideas-in-computer-architecture%2F</url>
    <content type="text"><![CDATA[By David A. Patterson Design for Moore’s Law The one constant for computer designers is rapid change, which is driven largely by Moore’s Law. It states that integrated circuit resources double every 18–24 months. Moore’s Law resulted from a 1965 prediction of such growth in IC capacity made by Gordon Moore, one of the founders of Intel. As computer designs can take years, the resources available per chip can easily double or quadruple between the start and finish of the project. Like a skeet shooter, computer architects must anticipate where the technology will be when the design finishes rather than design for where it starts. We use an “up and to the right” Moore’s Law graph to represent designing for rapid change. Use Abstraction to Simplify Design Both computer architects and programmers had to invent techniques to make themselves more productive, for otherwise design time would lengthen as dramatically as resources grew by Moore’s Law. A major productivity technique for hardware and soft ware is to use abstractions to represent the design at different levels of representation; lower-level details are hidden to offer a simpler model at higher levels. We’ll use the abstract painting icon to represent this second great idea. Make the common case fast Making the common case fast will tend to enhance performance better than optimizing the rare case. Ironically, the common case is often simpler than the rare case and hence is often easier to enhance. This common sense advice implies that you know what the common case is, which is only possible with careful experimentation and measurement. We use a sports car as the icon for making the common case fast, as the most common trip has one or two passengers, and it’s surely easier to make a fast sports car than a fast minivan. Performance via parallelism Since the dawn of computing, computer architects have offered designs that get more performance by performing operations in parallel. We’ll see many examples of parallelism in this book. We use multiple jet engines of a plane as our icon for parallel performance. Performance via pipelining A particular pattern of parallelism is so prevalent in computer architecture that it merits its own name: pipelining. For example, before fire engines, a “bucket brigade” would respond to a fire, which many cowboy movies show in response to a dastardly act by the villain. The townsfolk form a human chain to carry a water source to fire, as they could much more quickly move buckets up the chain instead of individuals running back and forth. Our pipeline icon is a sequence of pipes, with each section representing one stage of the pipeline. Performance via prediction Following the saying that it can be better to ask for forgiveness than to ask for permission, the next great idea is prediction. In some cases it can be faster on average to guess and start working rather than wait until you know for sure, assuming that the mechanism to recover from a misprediction is not too expensive and your prediction is relatively accurate. We use the fortune-teller’s crystal ball as our prediction icon. Hierarchy of memories Programmers want memory to be fast, large, and cheap, as memory speed often shapes performance, capacity limits the size of problems that can be solved, and the cost of memory today is often the majority of computer cost. Architects have found that they can address these conflicting demands with a hierarchy of memories, with the fastest, smallest, and most expensive memory per bit at the top of the hierarchy and the slowest, largest, and cheapest per bit at the bottom. Caches give the programmer the illusion that main memory is nearly as fast as the top of the hierarchy and nearly as big and cheap as the bottom of the hierarchy. We use a layered triangle icon to represent the memory hierarchy. The shape indicates speed, cost, and size: the closer to the top, the faster and more expensive per bit the memory; the wider the base of the layer, the bigger the memory. Dependability via redundancy Computers not only need to be fast; they need to be dependable. Since any physical device can fail, we make systems dependable by including redundant components that can take over when a failure occurs and to help detect failures. We use the tractor-trailer as our icon, since the dual tires on each side of its rear axels allow the truck to continue driving even when one tire fails. (Presumably, the truck driver heads immediately to a repair facility so the fl at tire can be fixed, thereby restoring redundancy!)]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>Computer Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RISC-V基本指令集概述]]></title>
    <url>%2F2019%2F07%2F06%2FRISC-V%E5%9F%BA%E6%9C%AC%E6%8C%87%E4%BB%A4%E9%9B%86%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[RISC-V的主要技术特性： 将ISA分离为一个小的基本ISA以及可选的扩展。 支持32位和64位地址空间。 使ISA扩展更为容易，包括紧密耦合以及松弛耦合的协处理器。 支持可变长的指令扩展。 提供有效率的现代标准的硬件支持，包括IEEE-754 2008浮点数标准以及C11和C++11语言标准。 用户ISA以及特权架构相互分离。 RISC-V包括三种基本的ISA：RV32I，RV32E以及RV64I。RV64I是64位地址空间标准的ISA，RV32I是32位地址空间标准的ISA，RV32E是RV32I的变种，寄存器的数量更少，面向一些深度定制的简单嵌入式系统。 RV32I基本ISARV32I是基本的32位整数ISA，只有47种指令。RV32I足以提供给现代操作系统足够的基本支持以及作为编译器的编译目标。指令集中有8个指令用于系统指令，可以被实现为单一的陷阱指令。剩下的指令可以分为三种：运算指令、控制指令以及内存交互指令。 RISC-V基于加载-存储结构，算术指令只能在寄存器上操作，内存中的数据只能读取和加载。 RV32I有31个通用整型寄存器，命名为x1 ~ x31，每个的位宽为32位。x0被命名为常数0，它也可以被作为目标寄存器来舍弃指令执行的结果。PC是寄存器x32。整个寄存器的组织如下： RV32I的指令长度为32位，并且需要在内存中对齐存储，并且是小端存储。一共有6种指令格式：R、I、S、U以及变种SB、UJ，如下所示： 可以发现SB、UJ与S、U格式的不同在与立即数编码方式的不同。 在这些指令格式当中，源寄存器最多可以有2个，命名为rs1以及rs2，目标寄存器最多有1个，命名为rd。一个重要的特性在于，这些寄存器标志的在指令中得位置都是一致的，这使得取寄存器的操作可以在指令译码的时候并行执行，从而优化了很多实现中的关键路径。 在立即数的实现当中也为了节省译码所需要的硬件电路复杂度而精心排布过。比如，立即数的第0位只有可能来自于指令的第7位（S类型）、第20位（I类型）或者常数0。立即数5只有可能来自于指令的第25位或者常数0，如此推断。 下面展示了RV32I的主要操作码表，可以发现RV32I的主要操作码的位数为5，而主操作码的长度为7位，但是在基础的ISA当中，最低两位被设置恒为11，保留这两位来作为ISA的扩展： 运算指令 RV32I包含有21条运算相关的指令，包括算术运算、逻辑运算以及比较运算。这些指令在整型寄存器上进行操作，一些指令需要立即数操作数。运算指令在有符号和无符号整数上都进行操作。有符号整数使用补码表示法来表示。所有的立即数操作数都是符号扩展的，即使是在立即数表示为无符号数的上下文当中。这个特性可以降低ISA的描述复杂度，在某些情况下也确实获得了更好的性能。 算术指令： ADD - [rd] &lt;- [rs1] + [rs2] SUB - [rd] &lt;- [rs1] - [rs2] SLL - [rd] &lt;- [rs1] L&lt;&lt; ([rs2] &amp; 0x1F) SRL - [rd] &lt;- [rs1] L&gt;&gt; ([rs2] &amp; 0x1F) SRA - [rd] &lt;- [rs1] R&gt;&gt; ([rs2] &amp; 0x1F) ADDI - [rd] &lt;- [rs1] + imm[11:0] SLLI - [rd] &lt;- [rs1] L&lt;&lt; shamt[4:0] SRLI - [rd] &lt;- [rs1] L&gt;&gt; shamt[4:0] SRAL - [rd] &lt;- [rs1] R&gt;&gt; shamt[4:0] 算术指令中的立即数都是有符号的。 逻辑指令： AND - [rd] &lt;- [rs1] &amp; [rs2] OR - [rd] &lt;- [rs1] | [rs2] XOR - [rd] &lt;- [rs1] ^ [rs2] ANDI - [rd] &lt;- [rs1] &amp; imm[11:0] ORI - [rd] &lt;- [rs1] | imm[11:0] XORI - [rd] &lt;- [rs1] ^ imm[11:0] 注意到，上面的逻辑指令中没有NOT指令，实际上，在RISC-V当中，实现NOT指令的方法是使用XORI将立即数设为-1，即可。这里就注意到了使用符号扩展的立即数（或者说将立即数都看作是补码表示的有符号数）的好处。在MIPS指令集当中，由于使用的是零扩展，因此需要额外增加一个指令NOR来实现这一功能。 比较指令： SLT - [rd] &lt;- ([rs2] &gt; [rs1]? 1 : 0), [rs1] and [rs2] are signed SLTU - [rd] &lt;- ([rs2] &gt; [rs1]? 1 : 0), [rs1] and [rs2] are unsigned SLTI - [rd] &lt;- (imm[11:0] &gt; [rs1]? 1: 0), [rs1] and imm[11:0] are signed SLTIU - [rd] &lt;- (imm[11:0] &gt; [rs1]? 1: 0), [rs1] and imm[11:0] are unsigned 上述指令有两个常用的习惯：使用SLTIU指令与立即数1来判断rs1是否等于0；使用SLTU，rs1设为x0来判断rs2是否不等于0。 最后，运算指令当中有两个比较特殊的指令，这两个指令使用的都是U类型的指令格式。 LUI - load upper immediate - 将寄存器rd的高20位设为指令中的20位立即数，而寄存器的低12位设为0。这个指令通常会和ADDI指令放在一起使用，用来将任意的32位常数拷贝到寄存器当中。它也可以和load以及store指令一起使用来加载或者存储32位静态地址的存储空间。 AUIPC - add upper immediate to PC - 将立即数中的高20位（也就是指令中的20位）加进PC当中，然后将结果写到寄存器rd中。AUIPC是RISC-V基址寻址机制的基础，以PC作为基址寄存器来进行寻址。 内存交互指令 RV32I提供5种从内存中读取一个整数到一个整型寄存器的指令，以及3种存储数据到内存当中的指令。所有的这些指令都使用字节地址的方式来定位内存中存储单元。指令中组成存储地址的方式是，将立即数地址的值（12位）与寄存器rs1中的值（32位）相加，实际上，很多时候寄存器rs1中的值的低12位都是0，因此实际上是两个地址的拼接而成为了一个32位的地址。 未对齐的加载和存储指令是显式支持的，但是不能保证它们能够自动执行或者高效地执行。 加载的指令全部使用I类型的指令格式。LW、LH、LB分别代表将32位字、16位半字、8位字节的数据拷贝到指定的寄存器当中： LW - [rd] &lt;- Mem(imm[11:0] + rs1) LH - [rd] &lt;- Mem(imm[11:0] + rs1) &amp; 0xFFFF LB - [rd] &lt;- Mem(imm[11:0] + rs1) &amp; 0xFF LH、LB会将rd的高位部分以符号位进行补全。而LHU、LBU这是用0来补全。 存储的指令全部使用S类型的指令格式。SW、SH、SB分别代表将32位字、16位半字、8位自己的数据存储到指定的内存单元中去： SW - Mem(imm[11:0] + rs1) &lt;- [rs2] SH - Mem(imm[11:0] + rs1) &lt;- [rs2] &amp; 0xFFFF SB - Mem(imm[11:0] + rs1) &lt;- [rs2] &amp; 0xFF 内存交互顺序RISC-V对于自身所要执行的对内存的加载和存储是可感知的，但是在多线程的环境当中，不能保证一个线程能够感知其他线程的内存交互操作。这种设计也称为松弛的内存模型。 在RV32I中，施加强制的内存访问顺序是显式提供的。RV32I提供FENCE指令来保证在FENCE指令之前和之后执行的内存访问指令是有序的。FENCE指令的格式如下： FENCE pred, succ pred和succ指的是在FENCE指令之前和之后的内存交互类型，包括R：内存加载、W：内存存储、I：设备输入、O：设备输出。举例： FENCE rw, w 上述指令表示，所有在FENCE指令之前的加载和存储指令一定在所有在FENCE指令之后的存储指令之前执行完毕。 RV32I同样还提供一个指令来同步指令流与内存访问：FENCE.I。使用该指令可以保证对指令内存的存储必定比FENCE.I之后的对指令内存取指令的操作先完成。 程序流控制指令 RV32I一共提供有6种指令来有条件的控制程序流。这些分支指令都是使用SB类型的指令格式，能够提供两个寄存器之间算术的比较且可以在1KB的地址范围内实现跳转。新地址由指令中符号扩展的12位立即数与当前的pc相加得到。 BEQ - jump to Addr(imm[12:1] + pc) if (rs1 == rs2) BNE - jump to Addr(imm[12:1] + pc) if (rs1 != rs2) BLT - jump to Addr(imm[12:1] + pc) if (rs1 &lt; rs2) BLTU - jump to Addr(imm[12:1] + pc) if (rs1 &lt; rs2) rs1 and rs2 are unsigned BGT - jump to Addr(imm[12:1] + pc) if (rs1 &gt;= rs2) BGTU - jump to Addr(imm[12:1] + pc) if (rs1 &gt;= rs2) rs1 and rs2 are unsigned 在RISC-V中，可以发现与其它RISC架构不同的地方是，很多其他的RISC架构在分支指令之后使用了分支延迟间隙。在Alpha和SPARC架构当中，为了使分支的结果尽可能早的计算得出，只允许使用简单的分支指令。在RISC-V的实现当中，将比较整合在了分支指令当中，这对于流水线的实现来说会造成分支的结果在流水线比较后的阶段才能计算出来。不过现代的处理器一般都有分支预测以及分支目标预测的功能，因此在这方面的平衡上，分支预测失败所造成的延迟开销与指令集合大小相比要小很多。 同样。RISC-V中也没有使用条件移动以及断言（predication）等技术。 RISC-V还提供了两种无条件跳转的控制语句：UJ型指令格式的JAL，jump-and-link指令，这个指令将pc设置为256KB地址空间范围中的任一地址，同时将原地址中的下一条指令地址（pc+4）写入寄存器rd中。因此这条指令可以用在函数返回当中。如果不需要rd，则可以使用x0作为rd来抛弃结果，这样的后果是指令变为了单纯的跳转指令JMP。 最后还有一个I类型指令格式的JALR指令，它提供了一个间接跳转的方式，跳转的目标为指令中的立即数作为低12位与rs1相加。这个指令被设计成与AUIPC一起使用来实现基于PC的基址寻址，地址范围为整个32位地址空间。 系统指令 RV32I中有8条系统指令。在简单的实现当中，可能会选择不实现（不使用）这些指令，并且在自身的系统软件中实现它们的功能，但是高性能的实现需要将它们在硬件中实现。 SCALL指令用于调用操作系统来实现系统调用。SBREAK指令用于调用调试器。 余下的6条指令用于提供对CSR寄存器组的读写。CSRs即control and status registers（控制状态寄存器组），提供一系列通用的工具给系统控制以及I/O使用: CSRRW - 从CSRs中拷贝其中一个CSR寄存器到一个通用寄存器当中，然后将rs1中的值覆盖给指定的CSR寄存器。 CSRRC - 从CSRs中拷贝其中一个CSR寄存器到一个通用寄存器当中，然后根据rs1的位模式对指定的CSR寄存器进行清零（如果rs1的某一位为1，则CSR寄存器中该位置为0。如果rs1的某一位为0，则CSR寄存器中该位不变）。 CSRRS - 从CSRs中拷贝其中一个CSR寄存器到一个通用寄存器当中，然后根据rs1的位模式对指定的CSR寄存器进行置位（如果rs1的某一位为1，则CSR寄存器中该位置为1。如果rs1的某一位为0，则CSR寄存器中该位不变）。 需要注意的是，在上述的CSR寄存器中，将数据拷贝到通用寄存器的过程中需要对数据进行零扩展。 CSRRWI、CSRRCI、CSRRSI的功能与CSRRW、CSRRC、CSRRS一致，只是将rs1替换成了5位的零扩展的立即数。 如果使用CSRRS指令，并且将rs1设为x0，那么功能相当于仅仅将CSR中的内容读取出来而不会改变rs1中的内容。如果使用CSRRW指令，并且将rd设置为x0，那么功能相当于写CSR。 在大多数的系统当中，CSRs只有在特权模式下才能访问，但RV32I提供了一些用户层次使用的基本CSR寄存器，这些CSR寄存器都是只读的并且只能够通过CSRR类指令来使用，这些CSR寄存器如下所示： cycle：用于记录从任意参照的时间流逝的时钟周期数。 time：用于记录流逝的系统时间。 instret：实时时钟。 从理想情况下来说，cycle、instret以及time这三个寄存器应该要有64位的位宽大小，因为32位的位宽会导致这三个寄存器迅速溢出。为了在32位的ISA中实现这一点，在CSRs中提供了三个高位寄存器cycleh，instreth以及timeh，这三个寄存器用于保存其对应的低位寄存器的高32位数据。在读取这64位数据的时候需要使用两个通用寄存器如x3:x2。]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>Computer Architecture</tag>
        <tag>RISC-V</tag>
        <tag>ISA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RISC-V异常与中断机制概述]]></title>
    <url>%2F2019%2F07%2F06%2FRISC-V%E5%BC%82%E5%B8%B8%E4%B8%8E%E4%B8%AD%E6%96%AD%E6%9C%BA%E5%88%B6%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[RISC-V的异常和中断机制相对来说是比较复杂的一个部分，但是它对于完整的处理器设计来说是必不可少的。在这里我对RISC-V的异常与中断的机制做一个简单的阐述，可能会有一定的错漏之处，欢迎批评斧正。 参考文献与资料： The RISC-V Instruction Set Manual Volume II: Privileged Architecture. https://riscv.org/specifications/privileged-isa/ FE310G: an open source RISC-V microcontroller – Interrupt System, http://embeddedsystems.io/fe310g-open-source-riscv-microcontroller-interrupt-system/ SiFive Proposal for a RISC-V Core-Local Interrupt Controller (CLIC), https://github.com/sifive/clic-spec/blob/master/clic.adoc#background-and-motivation 在此之前，首先要说明遣词用句中的问题。异常与中断实际上是很容易混用的词汇，一般来说，中断也是一种异常（广义上的），中断可以看作是来源于外部的异常。因此下文中所说的异常指的是狭义上的异常，即来源于核内部发生的异常（如：指令错误、ALU执行异常、写回存储器异常、长指令写回异常等）。而中断则是指广义上异常去除狭义上异常除外的所有异常，下文将会提到（包括外部中断、时钟中断以及软件中断）。 文章基于的是V1.9.1的RISC-V特权级架构。 RISC-V特权级别在任何时候，一个RISC-V的硬件线程（hardware thread, hart）都会运行在一个特权级别当中，这个特权级别作为一个模式编码在CSRs（control and status registers，控制状态寄存器组）当中。当前一共有三种RISC-V的特权级别： 级别 编码 名字 缩写 0 00 用户/应用级 U 1 01 特权级 P 2 10 (保留) 3 11 机器级 M 每个特权级别都有属于它的一个指令子集。机器级是最高的特权级别，可以运行任何指令，访问处理器中任何的一个寄存器。 不同类型设备对中断的使用方式不尽相同 高性能的类Unix系统 中断处理时间只占处理器时间中的很小一部分 快速的处理器核，智能设备 最小化的中断处理设备 软件调度 中低端的嵌入式系统 中断处理时间是处理器时间中显著的一部分 缓慢的处理器核 中断处理设备的设计是重要的一部分 中断处理器可以像任务管理器一样运作 高性能的实时系统 不能浪费时间在处理中断上 规律心跳计时的方式来处理IO设备请求 RISC-V中断的设计目标 简洁至上的原则。 支持所有类型的平台，从嵌入式系统中的微控制器到大型的虚拟服务器。 实现性能与实现成本之间的权衡。 灵活的支持特定的需求。 RISC-V中断类型RISC-V一共有两大类的中断类型：局部中断（Local Interrupts）以及全局中断（Global Inerrupts）。 局部中断是指直接与hart相连的中断，可以直接通过CSRs当中的xcause（mcause、scause、ucause）中的值得知中断的类型。在局部中断当中，只有两种标准的中断类型：计时中断（timer）以及软件中断（software）。 全局中断实际上就是外部中断（External Interrupts）。它与PLIC相连（Platform-Level Interrupt Controller，平台级中断控制器）。实际上全局中断在多个硬件线程的情况下最为常用。PLIC用于对外部中断进行仲裁，然后再将仲裁的结果送入核内的中断控制器。 RISC-V异常与中断处理的基本机制在RISC-V异常与中断处理的基本机制当中，会以简化的模型来进行阐述，也就是说，只考虑在机器级下异常与中断处理的基本机制。 有关的CSRsMachine Trap-Vector Base-Address Register (mtvec)mtvec，即机器模式异常入口基地址寄存器。mtvec是一个XLEN位（字长）的可读写的寄存器，保存异常向量的设置，包括向量的基地址（BASE）以及模式（MODE）。实际上它定义的就是异常入口程序的基地址。 其中WARL指的是Write Any Read Legal。mtvec寄存器是必须实现的，但是可以只包含硬编码的只读值。如果mtvec是可写的，那么寄存器中可以保存的值是可以有很多种的。BASE域中的值必须以4字节对齐，同时在MODE中设定的值可能会给BASE域的值带来额外的限制。 MODE域中的编码： 值 名字 描述 0 直接 所有的异常都将PC设为BASE 1 向量 异步的中断会将PC设置为BASE+4*cause >=2 (保留) Machine Cause Register (mcause)mcause，即机器模式异常原因寄存器。mcause是一个XLEN位的可读写的寄存器。RISC-V架构规定，所有的异常默认进入机器模式（M-mode），此时mcause被写入一个值表明是什么事件造成了这个异常。此外，mcause在实现中永远不会被写入，尽管它有可能会在软件中被显式要求写入。 当异常是由一个中断所造成的时候，mcause中的Interrupt位会被置为1。Exception Code域包含着标明最近一个异常发生的原因。下表列出了可能的机器级的异常编码。Exception Code域是WLRL（Write Legal Read Legal）的，因此它需要保证只能包含所支持的异常编码。 上表中也可以看出一些典型的异常和中断。 Machine Trap Value Register (mtval)mtval，即机器模式异常值寄存器。mtval是一个XLEN位的可读写寄存器。当一个异常发生进入机器模式时，mtval被写入该异常的信息，来帮助服务程序来处理这个异常。此外，mtval在实现中永远不会被写入，尽管它有可能会在软件中显式要求写入。 当一个硬件的断点触发，或者指令的获取，或者加载存储地址未对齐，或者页故障异常发生时，mtval会写入受异常影响的地址。在非法指令的异常当中，mtval会写入故障指令的XLEN位。对于其他的异常来说，mtval会写入为0。在将来可能会扩展更多的内容。 在RISC-V的指令获取异常当中，如果指令是变长的，mtval会包含一个指向该指令一部分的指针。而mepc会指向该指令的起始地址。 mtval还可以在非法指令异常时选择返回异常指令。（mepc指向内存中该异常指令的地址）如果不支持这个特性，那么mtval设置为0。如果支持了这项特性，在一个非法指令异常之后，mtval会包含整个异常指令。如果指令的长度小于XLEN，则mtval的高位使用0来填充。如果指令的长度大于XLEN，那么mtval会包含该异常指令的前XLEN位。 Machine Exception Program Counter (mepc)mepc，即机器模式异常程序计数器。mepc是一个XLEN位的可读写寄存器。mepc的最低位（mepc[0]）恒为0。在不支持16位指令扩展的实现当中，mepc最低的两位（mepc[1:0]）恒为0。 mepc是一个WARL的寄存器，必须能够包含所有合法的物理以及虚拟地址。它不需要支持包含所有可能的不合法的地址，某些实现当中，可能会将一些不合法的地址串转换成其他的不合法的地址来写入mepc当中。 当发生异常进入机器模式时，mepc会写入该异常的虚拟地址（没有实现MMU的就是实际的物理地址）。此外，mepc在实现中永远不会被写入，即使有些软件可能要求显式的写入。 Machine Status Register (mstatus)mstatus，即机器模式状态寄存器。mstatus是一个XLEN位的可读写的寄存器。下面展示的RV32位的格式。mstatus寄存器会追踪以及控制所有hart当前的状态。而在特权级以及用户级指令集架构当中则为sstatus以及ustatus寄存器。 mstatus当中不同特权级以及全局的中断使能栈在mstatus中，提供了不同特权级模式的中断使能位：MIE、SIE以及UIE。这些位用于表明当前特权级的全局中断使能情况。当一个hart在x特权级执行时，若xIE=1，则此特权级下允许中断。 为了支持嵌套中断，每个特权模式x都有一个两级的栈提供给中断使能位以及特权模式。xPIE保存着在异常发生前的中断使能位，xPP保存着异常发生前的特权模式。MPP有2位，SPP只有1位，而UPP是隐式为0的（异常进入用户模式只能是用户模式，异常进入特权模式可以是用户模式也可以是特权模式，异常进入机器模式则可以是所有的模式）。当一个异常发生后，从特权模式y切换到了特权模式x，则xPIE设置为xIE的值，xIE设为0，同时xPP设置为y。 而MRET、SRET以及URET指令用于从机器模式、特权模式以及用户模式下的异常返回。当执行xRET指令时，假设xPP为y。此时xIE从xPIE中恢复，特权模式设置为y，xPIE设置为1，xPP设置为U（或者M，如果用户模式不被支持）。 如果实现中没有用户模式，则UIE和UPIE硬编码为0。 Machine Interrupt Registers (mip and mie)机器模式下的中断寄存器有两个：Machine interrupt-pending register（mip，机器模式中断等待寄存器）以及Machine interrupt-enable register（mie，机器模式中断使能寄存器）。mip和mie都是一个XLEN位的可读写的寄存器，mip包含着与中断等待相关的信息，而mie包含中断使能位。在mip中，只有在低特权级别的位才能够使用CSR来寻址写入，包括：软件中断（USIP、SSIP）、时钟中断（UTIP、STIP）以及外部中断（UEIP、SEIP）。剩下的其他位都是只读的。 mip和mie在其他的特权级别下的寄存器分别为sip/sie以及uip/uie。如果一个中断通过设置在mideleg寄存器中的位，来下放到x特权级别下处理，则它在xip寄存器中是可见的，并且可以在xie寄存器中进行屏蔽。否则，对应的xip以及xie中的位会被硬编码为0。 MTIP、STIP、UTIP分别对应机器级、特权级以及用户级下的计时中断等待位。MTIP位是只读的，只能够通过写内存映射的机器模式计时器比较寄存器（machine-code timer compare register，mtimecmp）来清除。UTIP和STIP可以被工作在机器模式下的软件写入，用于传递计时中断给低级的特权级。用户级以及特权级的软件可以通过调用AEE（Application Execution Environment）和SEE（Supervisor Execution Environment）来分别清除UTIP和STIP位。 MTIE、STIE、UTIE分别对应机器级、特权级以及用户级下的计时中断使能位。 每个低级的特权级都有一个软件中断等待位（SSIP、USIP），可以使用在本特权级或者更高的特权级使用CSR指令来进行读写。机器级的MSIP只能通过存储器映射的控制寄存器来进行控制。 MEIP域是只读的，它表示一个机器模式下的外部中断正在等待。MEIP只能通过PLIC（Platform-Level Interrupt Controller）来设置。MEIE在设置后即允许外部中断。 SEIP是可读写的位。SEIP可能会在机器模式下被软件写入来表示在特权模式下有一个外部中断正在等待。此外，也可能是PLIC产生了一个特权级的中断正在等待，因此，特权级的外部中断可以由PLIC产生，也可以由机器模式下的软件来产生。UEIP与SEIP的实现相似。 MEIE、SEIE以及UEIE分别对应机器级、特权级以及用户级下的外部中断使能位。 对于所有的中断类型来说（软件中断、计时中断以及外部中断），如果其特权级没有得到支持，则对应的mip和mie中的位硬编码为0。 一个中断i会在mip以及mie中对应类型的位置为1时得以触发，并且此时全局的中断位也是1。默认情况下，当hart运行在低于机器模式的特权级别下时，或者hart运行在机器模式下且MIE位为1时，机器级的中断都是开放的。如果mideleg中的位i置为1，那么，中断在以下情况中是全局开放的：当前hart的特权级与下放的特权级相等（S或者U），且该模式的全局中断使能为为1（mstatus中的SIE或者UIE）；当前的特权级模式比下放的特权级模式要低。 Machine Timer Registers (mtime and mtimecmp)可以在硬件上提供一个实时的计数器，并通过内存映射的方式进行访问：machine-mode register - mtime，机器模式计时器寄存器。mtime必须运行在不变的频率当中，并且必须提供一个机制来决定mtime的基准时间。 mtime在RV32、RV64以及RV128中是64位的。同样还会提供一个内存映射的机器模式计时器比较寄存器：machine-mode timer compare register - mtimecmp。当mtime中的值比mtimecmp中的值要大的时候，就会产生一个计时中断。 在RV32当中，使用内存映射的方式写入mtimecmp只能是其32位的部分。下面的例程将一个新的比较值写入到mtimecmp当中： 12345# New comparand is in a1:a0li t0, -1sw t0, mtimecmp # 冲刷低32位，保证不会发生计时中断sw a1, mtimecmp+4 sw a0, mtimecmp 异常与中断的处理过程处理过程基于简化的模型，假设仅仅实现了机器模式的hart。 进入异常以下均为处理器硬件执行的行为： 更新mepc。如果出现的是中断，则mepc写入产生中断的指令的下一条指令的地址。如果出现的是异常，则mepc写入当前指令的地址。 更新mcause。根据产生异常的类型更新mcause，异常类型的编码上文已经提到。 更新mtval。某些异常需要将异常相关的信息写入到mtval当中。 更新mstatus。将异常发生前的MIE保存到MPIE当中，将异常发生前所处的特权级保存到MPP中，在本模型下恒为机器模式。将MIE设为0。这意味着在硬件上，RISC-V是不支持嵌套中断的。若要实现嵌套中断，则只能通过软件的方式来实现。具体实现：当一个异常发生后，则MPIE设置为MIE的值，MIE设为0，同时MPP设置为M。 跳转到mtvec中所定义的异常入口地址执行。如同上面所提到的，mtvec有两种模式，一种是直接模式，直接跳转到mtvec中的基地址执行。另一种是向量模式，根据mcause中的异常类型跳转到对应的异常处理程序首地址中执行。 退出异常当异常处理程序执行完毕后，在程序最后会调用MRET指令来退出异常处理程序（其他特权级的指令为SRET、URET，本模型不需要）。执行MRET指令后处理器硬件执行的行为如下： 从mepc中定义的地址执行。恢复到异常发生前的程序流执行。 更新mstatus。将异常发生前的mstatus的状态恢复，如上文mstatus描述中所述。具体实现：此时MIE从MPIE中恢复，特权模式设置为M，MPIE设置为1，MPP设置为M。 总结RISC-V模式下的异常与中断处理的过程为（仅实现机器模式）：]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>Computer Architecture</tag>
        <tag>RISC-V</tag>
        <tag>ISA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[寄存器重命名技术]]></title>
    <url>%2F2019%2F07%2F06%2F%E5%AF%84%E5%AD%98%E5%99%A8%E9%87%8D%E5%91%BD%E5%90%8D%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[在计算机体系结构当中，寄存器重命名是一种技术来消除数据依赖的方法，通过重用一组连续的指令所使用的寄存器，它们之间不存在数据依赖。通过消除这些负面的数据依赖，可以揭示在指令流中更多的指令级别并行执行的相关内容，从而可以被多种技术作为补充以获得更好的性能，如超标量以及乱序执行处理器。 问题描述在寄存器机中，程序由操作数据的指令构成。指令必须对这些值命名来分辨它们。一种典型的指令会说：将X和Y的值相加并且将结果放在Z中。在这条指令当中，X，Y，和Z都是存储位置的名字。 为了获得紧凑的指令编码，绝大多数的处理器指令集拥有一个小的特殊存储位置的集合可以被直接命名（通用寄存器）。比如，在x86指令集架构当中，有8个整型寄存器，x86-64有16个，大多数的RISC机有32个，IA-64有128个。在小型的处理器当中，这些寄存器一般组合寄存器组的机构。 不同的指令可能会需要执行不同的时间。比方说，一个处理器可能能够在一个从主存中读取数据的指令所需要的时间里，执行上百条其他的指令。因此，这些执行速度更快的指令会在load指令执行完成前先执行完毕，这时候指令真正执行的顺序就会和原来的程序有出入。乱序执行已经在现在的高性能CPU上使用了。 考虑在一个乱序执行的CPU上执行下列指令序列： 12345601 R1 = M[1024]02 R1 = R1 + 203 M[1032] = R104 R1 = M[2048]05 R1 = R1 + 406 M[2056] = R1 指令4,5,6与指令1,2,3是相互独立的，但是指令4不能在指令3完成前先完成，否则指令3会写入错误的值。这样的约束可以通过重命名寄存器的名字来消除： 12301 R1 = M[1024] 04 R2 = M[2048]02 R1 = R1 + 2 05 R2 = R2 + 403 M[1032] = R1 06 M[2056] = R2 现在，指令4,5,6可以与指令1,2,3并行执行，因此这个程序现在可以执行的更快了。 如果可能的话，编译器会检测不同的指令然后尝试去给它们分配不同的寄存器。然而，这里只有有限的寄存器名字可以在汇编代码中使用。很多高性能的CPU有更多的物理寄存器可以在指令集中被直接命名，因此在硬件中进行重命名寄存器操作可以获得额外的并行性。 数据冒险数据冒险有三种：RAW、WAW以及WAR（分别是什么意思不再赘述，可以参考RISC-V构建相关笔记参考或者直接google即可），下面阐述的是另外一种表述的方式： 写后读（Read-after-write，RAW）：从寄存器或者内存中读取的数据，必须是先前最后一个写指令写入的数据。这种依赖也称为直接依赖或者流依赖（或者真依赖？，原文：ture dependency or flow dependency）。这种依赖（冒险）的解决方法可以通过使指令按照程序顺序来执行。 写后写（Write-after-write，WAW）：连续写入同一个寄存器，寄存器的最终结果必须等于最后一次写入的值。这种依赖也称为输出依赖。这种依赖（冒险）的解决方法可以通过取消（无效）先前的写指令来解决。 读后写（Write-after-read，WAR）：读操作获得的数据是此前写入的，而不是此后写操作的结果。这种依赖也称为假依赖（false dependency），可以通过重命名来解决。WAR依赖也可以被称为反依赖（anti-dependencies）。 不同于在所有的读完成之前都将写操作进行延时，目标存储地址的两个拷贝可以被维护：旧的以及新的拷贝。这样，在写之前的读就可以提供旧的值，而在写之后的读就可以提供新的值。这样做，假依赖被大破，并且额外的乱序执行的机会得以创造出来。当所有的读取都已经完成了，旧的值就可以被丢弃。这是寄存器重命名的必要概念。 所有读取以及写入的位置都可以被重命名。最常见的是重命名通用寄存器以及浮点寄存器，标志以及状态寄存器甚至独立的状态位也可以被重命名。 存储器重命名同样可行，虽然没有寄存器重命名那么常见。在Transmeta Cursoe的处理器中门存储缓存可以使用存储器重命名技术。 如果程序可以不立即重用同一个寄存器，那么就没必要进行寄存器重命名了。一些指令集提供大量的寄存器就是为了这个目的。但是这种做法也有它的局限性： 编译器很难完全避免在不导致程序尺寸大增的同时避免重用寄存器。在循环当中，连续的迭代执行需要复制循环体的代码以使用不同的寄存器，这种技术叫做循环展开。 大量的寄存器需要在指令中使用更多的比特位来指定寄存器，这样也会增大代码量。 很多指令集在此之前只有比较少的寄存器，同时在新的架构中实现也不能扩大寄存器的数量（没有向后兼容的特性）。 代码量的增加是很重要的，因为当程序代码变大之后，指令缓存未命中的概率会增大，处理器停滞以等待新指令的时间会变多。 体系结构寄存器与物理寄存器很多的机器语言程序所指定的用于读写的寄存器都受限于在指令集中指定的寄存器。举例来说，在Alpha当中声明了32个整数寄存器，每一个都是64位宽，以及32个浮点寄存器，每一个也是64位宽。这些是体系结构寄存器。使用Alpha指令集写的程序指令会指定读写这64个寄存器。 Alpha 21264（1996年由DEC设计的RISC处理器），实现了Alpha指令集，且物理上拥有80个整型寄存器以及72个浮点寄存器。（事实上，还有更多的寄存器，只是这些寄存器都不能用于寄存器命名操作） 接下来就描述两种寄存器命名的类型，可以利用为数据提供执行的电路来进行区别。 在所有的重命名的策略当中，机器会将体系结构寄存器根据指令流转换成标签。体系结构寄存器可能会使用3到5个比特位来进行描述，而标签通常使用6到8个比特位来进行描述。重命名的部件必须拥有读取的端口对应每个时钟周期中重命名的每个指令的每个输入，同时还必须拥有一个写入的端口对应每个时钟周期中重命名的每个指令的每个输出。因为寄存器组的大小一般来说都会以读写端口的个数平方增长，而重命名的器件一般会在物理上要更大并且拥有更大的功率。 在标签索引的寄存器组类型（tag-indexed register file）中，存在一个大的寄存器组给数值存放，所有的寄存器都可以打上标签。举例来说，如果机器拥有80个物理寄存器，那么这里应该使用7位的标签。在这个例子中，有48个可能的标签值没有被使用。 在这种类型当中，当一个指令发射给一个执行单元的时候，这个源寄存器的标签会发送给物理的寄存器组，这些标签对应的值会被发送给执行单元。 在保留站（reservation station）的类型当中，有很多小的相关的寄存器组，通常在每个执行单元的每个输入都有一个。发射队列中的每条指令的每个操作数对应着这个物理寄存器堆的一个存储位置。 在这种类型当中，当一条指令发射给一个执行单元的时候，执行单元对应的寄存器堆的相应条目被读出发送给执行单元。 体系结构寄存器组与隐退寄存器组（Retirement Register File，RRF） 存储了被提交了的寄存器状态的寄存器组。存储了被提交的体系寄存器的状态。通过逻辑寄存器的号来查询这个寄存器组。重排序缓冲区（reorder buffer）中的引退（retired）或者说提交（committed）指令，把结果写入这个寄存器组。 远期寄存器组（Future File） 处理器对分支做预测执行的寄存器的状态保存于此。使用逻辑寄存器号来索引访问。在Intel P6微体系结构，称之为Active Register File。 历史缓冲区（History Buffer） 一般来说和远期寄存器组一起使用。包含已经被覆写的寄存器的旧的值。如果分支预测失败，使用历史缓冲区中的数据来恢复流水线。 重排缓冲区（Reorder Buffer，ROB） 为了实现指令的顺序提交，处理器内部使用了一个Buffer。如果在该缓冲区中排在一条指令之前的所有都已经提交，没有处于未提交状态的（in flight），则该指令也被提交。因此重排缓冲区是在远期寄存器组之后，体系结构寄存器组之前。提交的指令的结果写入体系寄存器堆。 重排缓冲区可以是data-less或者是data-ful的。 在Willamette的ROB结构当中，ROB的每个项都指向一个物理寄存器组（Physical Register File，PRF）中的寄存器，同时还包括一些簿记数据。这是第一种乱序设计，由Andy Glew在Illinois使用HaRRM完成。 在Intel P6的ROB结构当中，ROB的每个项都包含数据，而没有分离的PRF结构。来自ROB的数据在提交后会被复制到RRF。 标签索引寄存器组的实现细节这种重命名的方式在MIPS R10000（实现MIPS IV指令集的RISC处理器，在1996年由MIPS科技有限公司设计）、Alpha 21264以及AMD Athlon（AMD在1999年中期到2005年生产的x86处理器）的浮点运算部件中使用。 在重命名的阶段当中，每个被引用的体系结构寄存器（不论是读还是写）按其体系结构索引号到重命名文件（remap file）中查找。这个文件会返回一个标签以及一个准备位。如果有一个队列中的指令要写这个寄存器，且还没有完成，那么这个标签还没准备好，还不能被使用。 对于读取操作来说，这个标签将会取代在指令中的体系结构寄存器，也就是说，写后读的数据相关必须恪守，读操作只有在该标签的ready位是就绪的时候才能执行。 对于写入操作来说，从一个FIFO的空闲标签队列中取出一个标签，然后一个新的映射条目写入重命名文件，未来的读取该体系结构寄存器的指令将指向这个新的tag。在这种情况下，可以发现，WAW是一种寄存器数据的假依赖，用这种重命名的方法就可以去掉假依赖。因为以前为该体系结构寄存器分配的物理寄存器被保存在指令的重排缓冲区，根据写入的tag（此时tag未就绪，写操作尚未执行），由于重拍缓冲区是一个FIFO的队列，根据指令的阶码顺序安排指令引退，只将最后一条有效的写的指令执行即可。 操作数寄存器被重命名后的指令将被放入不同的发射队列（issue queues）。这些指令等待所需的各种资源（源操作数对应的物理寄存器）就绪。 当发射队列中的某条指令的素有操作数都是就绪的，这条指令就是发射就绪。在每个周期，发射队列挑选出一些就绪的指令，发射到功能单元。未就绪的指令仍然留在发射队列中。这种从发射队列中无序删除指令，使得发射队列的电路实现占用面积大、功耗高。 被发射的指令读取源操作数的tag索引在物理寄存器堆中对应的物理寄存器（忽略掉刚刚公告过的操作数），然后开始执行指令。 指令执行结果写入目的操作数的tag索引在物理寄存器堆对应的物理寄存器，同时公告给每个功能单元输入端的旁路网络（bypass network，即把执行结果“直通”给流水线各个步骤的中间缓冲）。 写寄存器的指令在引退时，把被写的目的操作数寄存器使用过的上一个tag放入“空闲tag队列”中，使得它可以被其它被解码的指令重用。而该指令的目的操作数寄存器当前对应的tag仍然被占用，因为后面可能还有指令需要读取当前tag对应的物理寄存器的内容。 一个异常（将导致中断）或者分支预测失败导致了重命名文件退回到最后一条有效的指令的重命名状态，通过组合状态的快照（在历史缓冲区）与重排缓冲区中等待顺序引退的指令的以前用过的tags。这种机制可以实现恢复任意时刻的重命名状态。 保留站的详细实现保留站的实现在AMD K7以及K8的整数执行部分中使用。 在重命名阶段中，每个体系结构寄存器会从远期寄存器组以及重命名文件中查找对应的物理寄存器。如果没有写指令还没有完成写入该物理寄存器，则说明这个源操作数已经就绪。当这条指令被放入发射队列，从远期寄存器堆相应的物理寄存器读出内容放入保留站中对应的条目。指令对目的寄存器的写入，在重命名文件中的产生了一个新的、未就绪的tag。tag数通常是按照指令的顺序分配，因此不需要空闲tag先进先出队列。 如同tag索引模式，发射队列中的未就绪操作数等待匹配的tag公告。但不同于tag索引模式，tag的匹配导致对应的内容数据写入发射队列对应的保留站的条目。 被发射的指令从保留站读取它的操作数，忽略掉那些刚刚公告过的操作数，然后开始执行。保留站寄存器堆通常很小，可能只有8个条目。 指令执行结果写入重排缓冲区，以及保留站（如果发射队列有匹配的tags），以及远期寄存器堆。 指令引退时，复制重排序缓冲区中的值到体系结构寄存器堆。体系结构寄存器堆用于从异常或者分支预测失败时恢复。 在指令引退时可以识别出异常与分支预测失败，引起体系结构寄存器堆覆盖掉远期寄存器堆的内容，并标记重命名文件中所有寄存器都是就绪。通常，没有办法为一条处于解码与引退之间的指令恢复远期寄存器堆，因此通常没有办法在更早期为分支预测失败做恢复工作。 比较两种策略在两种策略中，指令都是有序的插入发射队列当中，但是以乱序移除。需要考虑的是，当多条指令都可以发射的时候，需要一个或者多个优先级的编码方式。 保留站具有更好的延迟性能，因为保留站直接获取数据，而不是根据标签再获取数据。 保留站具有更好的从指令发射到执行的延迟性能。因为每个本地寄存器堆远小于那种大型的用tag索引的中央寄存器堆。tag产生与异常处理也更为简单。 与tag索引的简单的寄存器堆相比，保留站的物理寄存器堆的总规模更大，功耗更大，更为复杂。更糟糕的是，每个保留站的每个条目可以被每条结果总线写入。例如，每个功能单元具有8条发射队列条目的处理器，相比于tag索引的模式有9倍的旁路网络，结果直通（forwarding）需要更大的功耗与面积。 保留站模式在4个位置（远期寄存器堆，保留站，重排序区、系统结构寄存器堆）保存结果值。而tag索引模式只需要在物理寄存器堆保存结果值。由于结果值来自功能单元，保留站模式必须公告结果到许多存储位置，用掉了非常多的功耗、面积、时间。如果处理器具有非常精确的分支预测、非常关注执行延迟，保留站也是个很好的选择。]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>Computer Architecture</tag>
        <tag>Processor</tag>
        <tag>Out-of-order Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomasulo算法]]></title>
    <url>%2F2019%2F07%2F06%2FTomasulo%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[托马苏洛算法（Tomasulo’s algorithm）是一种计算机硬件架构的算法，用于动态调度指令的指令，允许乱序执行以及更有效率的使用多个执行单元。它在1967年由罗伯特-托马苏洛在IBM发明，并且在IBM的IBM System/360 Model 91’上的浮点运算单元进行了实现。 托马苏洛算法的主要创新点在于算法包含了硬件上的寄存器重命名（register renaming）、用于所有执行单元的保留站（reservation stations），以及一个公共的数据总线（common data bus，CDB），用于广播给所有的需要计算得到的数据的保留站。该算法使得对指令的并行执行效率得到了提高，但是另一方面会在使用计分板或者其他更早的算法时发生停滞。 罗伯特-托马苏洛在1997年获得了埃克特-莫齐利奖（Eckert–Mauchly Award，授予给对数字系统以及计算机体系结构作出贡献的奖项） 参考资料： Computer Architecture: A Quantitative Approach, 3.4 Overcoming Data Hazards with Dynamic Scheduling Wikipedia: https://en.wikipedia.org/wiki/Tomasulo_algorithm 概览托马苏洛算法有以下几点重要的特征： 通过保留站实现的寄存器重命名技术，并且保留站的空间比寄存器的数目要多。 通用数据总线（Common Data Bus，CDB）的使用，广播到每个保留站、寄存器组以及存储缓冲区。 原程序顺序中分支指令后续的指令，只能在分支指令执行完毕后才能继续执行。进入issue状态指令无法退出。 乱序处理器的一个重要课题是，区别于顺序执行的流水线处理器，它对[数据依赖]的处理变得十分重要。数据依赖一共有3种：RAW、WAR以及WAW。在顺序执行的流水线处理器当中，只会有RAW真依赖的发生，可以通过旁路或者stall的方法来解决。但是在乱序处理器当中，还会有WAR以及WAW的依赖。然而，WAR和WAW实际上是一种名字依赖，也就是说，通过重命名寄存器可以使得数据依赖得以消除。可以通过一个例子来说明寄存器重命名是怎么消除名字依赖的： 考虑下面的MIPS64汇编代码： 12345DIV.D F0, F2, F4ADD.D F6, F0, F8S.D F6, 0(R1)SUB.D F8, F10, F14MUL.D F6, F10, F8 上述的指令序列当中，存在有3处的名字依赖： ADD.D与MUL.D，对于F6存在WAW依赖。 ADD.D与SUB.D，对于F8存在WAR依赖。 S.D与MUL.D，对于F6存在WAR依赖。 这些依赖都可以通过寄存器重命名来进行解决： 12345DIV.D F0, F2, F4ADD.D S, F0, F8S.D S, 0(R1)SUB.D T, F10, F14MUL.D F6, F10, T 需要注意的是，F8接下来的代码也要使用T来进行代替，同时还需要考虑跳转到这个代码的branch指令的依赖性问题。但是这都是在编译器层面所做的重命名工作，在托马苏洛的算法当中，寄存器的重命名都是在硬件层次上完成的。 托马苏洛算法对寄存器命名的实现机制是使用保留站。每个功能部件都拥有一个保留站，每条指令发射之后会进入保留站。一个基本的概念就是：保留站中的条目比物理寄存器的数目要多，因此可以通过重命名来解决WAR和WAW依赖。 使用保留站会带来两个重要的特性： 冒险检测以及执行控制是分布式的，每个保留站将会决定哪条指令可以执行。 CDB可以广播到任意一个保留站，这是一种旁路的实现方式：如果在一个保留站当中的指令的其中一个源操作数还在等待其他指令的执行结果，那么这个保留站将会监测CDB的状态，当执行结果出现在CDB时，就能写入到保留站对应指令的条目当中。 上图展示的是一个使用托马苏洛算法的基础的MIPS浮点运算单元结构图。指令队列是一个FIFO的队列，从指令单元将程序指令顺序读入到队列当中。两个浮点运算单元：加法器以及乘法器都各有一个保留站，保留站中存放有发射了的指令以及实际的源操作数，以及用于冒险检测和执行控制的信息。存储缓冲区和加载缓冲区中都有保存地址以及数据的条目，加载缓冲区有3个主要的功能：保存等待有效地址计算完成的load指令条目、保存等待内存有效的load指令条目以及保存等待CDB可用的数据。存储缓冲区的功能相似。执行单元的结果全部放到CDB当中，CDB有多个去向（多个广播目标）：存储缓冲区、所有的保留站以及寄存器组。 下面讨论在托马苏洛算法中3个执行阶段的执行过程： 发射（issue） 在指令队列当中的指令，按照FIFO的顺序出队。如果队首的指令所要使用的执行单元的保留站有空闲的空间，该指令则发射。否则，则会发生结构冒险，此时需要等待目标的执行单元的保留站有空闲的空间为止。发射指令到保留站的时候，会对指令所需要的源操作数进行跟踪。如果源操作数还没准备好，则会在保留站中写入相关的信息跟踪该源操作数，重命名也在这个时候发生，解决WAR以及WAW冒险。 执行（execute） 在保留站中的指令，如果源操作数还不可用，保留站会监控CDB，当所需要的源操作数出现时加载入保留站对应条目当中。当所有源操作数都可用的时候，指令就会执行。这个过程解决的RAW冒险。需要注意的是，在保留站中，可能会出现多条指令在同一个时钟周期内都可行的情况，这时只能够由对应的执行单元来决定执行哪一条指令。对于加载和存储指令来说，执行的执行需要两步：计算内存地址、加载/存储数据。为了避免不必要的冒险情况发生，load和store指令的执行需要按照源程序的顺序来执行。最后，对于分支情况，规定原执行顺序中在分支指令后的指令，需要等待分支执行完毕后才能执行。 写回（Write result） 执行完毕后执行单元的结果写入到CDB当中，CDB可以将结果发送到任意一个保留站以及寄存器组。存储指令所需要的地址和数据写入到存储缓冲区当中，当内存空闲时则写入。 保留站用于检测数据冒险以及控制执行。保留站中的每个条目都有一个标签（tag），标签的数目比物理寄存器的数目要多。因此，通过在保留站中对指令源寄存器的重命名，可以解决数据冒险的问题。 托马苏洛算法和计分板算法的重要区别在于：托马苏洛算法中，指令发射到保留站后，源寄存器号都会丢弃掉，取而代之的是保留站的标签或者是真实的值。 下面给出托马苏洛算法描述所需要的记号，这些信息在硬件中会实际实现： Op：指令所要执行的操作。 Qi，Qk：产生指令所需要的源操作数的保留站标签（产生对应结果的指令）。0表示真实的值已经存入保留站当中。 Vi，Vk：源操作数的值。对于load来说，Vk是立即数。 A：保存地址计算所需要的信息。对于load和store指令来说，执行前A存放的是立即数，执行后A存放的是有效地址。 Busy：标志该保留站以及对应的执行单元是否忙。 Qi：在寄存器组当中，标志将要写入到该寄存器的是哪个保留站Qi中的指令。0表示没有在保留站中的指令需要写入该寄存器。 例子下面给出一段程序例子，以及一系列的图表来说明算法执行的过程： 123456L.D. F6, 32(R2)L.D. F2, 44(R3)MUL.D. F0, F2, F4SUB.D. F8, F2, F6DIV.D. F10, F0, F6ADD.D. F6, F8, F2 第一个表给出的时刻，是所有指令发射后，第一条load指令写回完毕，第二条load指令执行完毕的情况： 可以注意到，在DIV.D.以及ADD.D.指令之间存在WAR的冒险。通过上面的表格，我们可以说明其解决的方式：DIV.D.的F6源寄存器的数据，来自于第一条load的指令。那么就有两种情况：如果load执行完毕，那么其值会直接写入到乘法器的保留站当中；如果load还没执行完毕，则DIV.D.保留站条目中的Qk域会写入Load1。这两种情况，DIV.D.和ADD.D.的执行都是相互独立的。 下面给出进行到MUL.D.指令执行完毕后的情况： 在之后的讨论中，假设load需要1个时钟周期，add需要2个时钟周期，multiply需要6个时钟周期，divide需要12个时钟周期。 详细算法描述下面给出托马苏洛算法的详细描述，使用伪代码。首先引入几个记号： RegisterStat：寄存器组中记录寄存器写入的情况，相当于上述的Qi。如RegisterStat[rs].Qi指寄存器rs被写入的情况。 RS：保留站条目，如RS[r]表示保留站r中对应指令的条目。 Regs：寄存器组。 issue stage123456789101112131415161718192021222324252627282930313233343536373839404142434445/* 算术指令 - 等待保留站r可用后，执行*/// 如果源寄存器rs等待其他指令写入if (RegisterStat[rs].Qi¦0) &#123; // 追踪所要写入的指令 RS[r].Qj ← RegisterStat[rs].Qi&#125;else &#123; // 否则将寄存器中真实的值写入保留站 RS[r].Vj ← Regs[rs]; RS[r].Qj ← 0;&#125;// 如果源寄存器rs等待其他指令写入if (RegisterStat[rt].Qi¦0) &#123; RS[r].Qk ← RegisterStat[rt].Qi;&#125;else &#123; RS[r].Vk ← Regs[rt]; RS[r].Qk ← 0;&#125;RS[r].Busy ← yes;// WAW依赖会在这里得到解决：发射完成的指令序列中，只有最后一个写入有效，其他会被丢弃RegisterStat[rd].Q ← r;/* 存储或者是加载指令 - 等待缓冲区r可用后，执行 */// 公共部分if (RegisterStat[rs].Qi¦0) &#123; RS[r].Qj ← RegisterStat[rs].Qi;&#125;else &#123; RS[r].Vj ← Regs[rs]; RS[r].Qj ← 0;&#125;RS[r].A ← imm;RS[r].Busy ← yes;// load指令RegisterStat[rt].Qi ← r;// store指令if (RegisterStat[rt].Qi¦0) &#123; RS[r].Qk ← RegisterStat[rt].Qi;&#125;else &#123; RS[r].Vk ← Regs[rt]; RS[r].Qk ← 0&#125;; execute stage12345678/* 算术指令 - 等待&#123;(RS[r].Qj = 0) and (RS[r].Qk = 0)&#125;为真 */// 计算结果/* 加载/存储指令步骤1 - 等待&#123;RS[r].Qj = 0&#125;为真，并且r是加载存储队列的队首 */RS[r].A ← RS[r].Vj + RS[r].A // 计算地址/* 加载指令步骤2 - 等待步骤1执行完毕 */// 从Mem[Rs[r].A]中读取数据 write result stage123456789101112131415161718192021/* 算术指令或加载指令 - 等待执行结束以及CDB可用 */// 写入寄存器∀x(if (RegisterStat[x].Qi = r) &#123; regs[x] ← result; RegisterStat[x].Qi = 0&#125;);// 写入等待第一个源操作数的保留站∀x(if (RS[x].Qj = r) &#123; RS[x].Vj ← result; RS[x].Qj ← 0; &#125;);// 写入等待第二个源操作数的保留站∀x(if (RS[x].Qk = r) &#123; RS[x].Vk ← result; RS[x].Qk ← 0;&#125;);RS[r].Busy ← no;/* 存储指令 - 等待执行结束以及&#123;RS[r].Qk = 0&#125;为真 */Mem[RS[r].A] ← RS[r].Vk;RS[r].Busy ← no; 影响托马苏洛算法中保留站、寄存器重命名以及CDB的技术在设计高性能的计算机中得到了明显的性能提升。虽然在360/91后很多年都未被使用，但是从1990年开始许多超标量处理器的动态调度策略基于托马苏洛算法。原因有多个： 虽然算法提出时高速缓存还没有被设计出来，但是乱序执行允许缓存不命中时继续执行其他的指令，可以把缓存未命中隐藏起来。 随着处理器要求性能的提高（同时执行指令数的提升以及对非数字操作指令的要求），寄存器重命名、动态调度等技术变得越来越重要。 不需要编译器来进行优化调度，寄存器重命名在硬件层次上进行。]]></content>
      <categories>
        <category>Computer Architecture</category>
      </categories>
      <tags>
        <tag>Computer Architecture</tag>
        <tag>Processor</tag>
        <tag>Out-of-order Architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Quine-McCluskey算法]]></title>
    <url>%2F2019%2F07%2F06%2FQuine-McCluskey%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[奎恩——麦克拉斯基算法是对布尔函数最小化的一种方法，由Willard V. Quine提出并由Edward J. McCluskey进行扩展。它在功能上与卡诺图相同，但是它的表格形式使得它能够在计算算法中表现的更有效率。同时它还提出了一种确定性的方法来检查是否是最小化的布尔函数。 该算法包含两步： 找到函数的所有素蕴涵项（乘积项） 使用素蕴涵项图表（包含上一步找到的所有素蕴涵项）来找到函数中本质素蕴涵项，对覆盖这个函数是必须的其他素蕴涵项也同样要使用。 复杂度尽管该算法比卡诺图的最小化要更为实用（4个变量以上的情况下），奎恩——麦克拉斯基算法是NP完全的，该算法的执行时间会随着变量数量的增长而指数上升。对于一个有n个变量的布尔函数来说，它的素蕴含项的个数能达到$3^nln(n)$。有大量变量的函数必须使用潜在的非最优的启发式方法来最小化，如Espresso Logic Minimizer。 举例在这个例子当中，函数的输入是4个变量：$f:{0,1}^4 \to {0,1}$，并且有： $$ f(A,B,C,D)=\Sigma m(4,8,10,11,12,15)+d(9,14) $$ 该函数中，9和14的值不关心。 第一步：寻找素蕴含项 首先，我们将函数用真值表的形式表示： 通过真值表可以很容易的写出乘积项的和式： $$ f_{A,B,C,D} = A’BC’D’+AB’C’D’+AB’CD’+AB’CD+ABC’D’+ABCD $$ 显然，这个函数表达式不是最小化的。为了进行最优化，首先将所有的乘积项放进一个最小项表当中，不关心的项同样放进这个表里面： 现在，我们还可以将最小项之间进行合并。如果两个最小项只有一个比特是不同的，那么该比特可以使用一个“-”来表明该比特不需要考虑。不能合并的最小项使用“*”来进行标记。将“-”看作是第3种位值即可： 第二步：构造素蕴含项表 此时所有的最小项都不能再合并了，因此构建一个寻找本质素蕴含项的表： 在这个表中，查找只有一个“X”的列，如果有合并的最小项具有这样的列，那么这个最小项就是本质素蕴含项。在上述例子当中，本质素蕴含项就是m(4,12)以及m(10,11,14,15)，其他两个素蕴含项都可以被其他的蕴含项所覆盖。因此，上述的布尔函数可以简化为： $$ f_{A,B,C,D}=BC’D’+AB’+AC $$ 或者 $$ f_{A,B,C,D}=BC’D’+AD’+AC $$]]></content>
      <categories>
        <category>FPGA</category>
      </categories>
      <tags>
        <tag>Computer Architecture</tag>
        <tag>FPGA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HLS简介]]></title>
    <url>%2F2019%2F07%2F06%2FHLS%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[High-level Synthesis(HLS)最早期的芯片设计由于其结构简单的特性，设计人员往往可以直接使用半导体来进行手工设计。随着芯片复杂度的提高，设计人员开始往抽象的设计方式进行过渡。上世纪80年代的Verilog/VHDL语言是抽象化的第一步，由此带来的是芯片硬件复杂度的指数级增长。RTL（register-transfer evel）级的描述则是抽象程度的又一次提高，它使得设计者可以只关注寄存器以及在寄存器上的操作，而不需要关心寄存器是如何实现的。EDA工具（electronic design automation tools）可以先把RTL转化成数电模型，再由模型转换成一个设备上的具体电路实施方案。所谓“方案”其实就是编译出的文件，这些文件可以用于规定某个自定义设备，也可以用于编程一些现有的设备，比如FPGA（field-programmable gate array）。 高层次综合（High-level Synthesis，HLS）是抽象程度的又一次提升，它允许设计者着眼于更大的体系结构性的问题而不是单一的寄存器中每个时钟周期的操作。在HLS设计中，设计者所要关心的行为不包括特定的寄存器或者时钟特性，并且一个HLS工具负责产生RTL级的微结构。早期的HLS工具都是基于行为式的Verilog，并且通过Verilog产生RTL。如今很多商用的HLS工具使用C/C++作为前端的语言。 总的来说，现在HLS可以自动完成以前RTL设计师需要手动完成的工作包括： HLS自动分析并利用算法中的潜在并发性。 HLS自动在需要的路径上插入寄存器，并自动选择最适合的时钟。 HLS自动产生控制数据在一个路径上出入方向的逻辑。 HLS实现的接口会自动连接到系统的其他部分。 HLS将数据映射到储存单元来平衡资源的使用以及带宽。 HLS自动将程序中计算的部分对应到逻辑单位，在实现等效计算的前提下自动选取最有效的实施方式。 一般来说，HLS的目标就是使得设计者所提供的输入以及一些限制条件来替设计者完成很多的决定。 然而，HLS工具还不能强大到能处理任何的软件代码。很多在软件编程中所使用的概念在硬件设计当中是很难实现的。一般来说，HLS需要设计者在代码中添加一些额外的信息（如#pragmas）来使工具能够生成更为高效的设计。HLS一般来说无法处理动态内存分配，对标准库的支持也很少。系统调用在硬件设计中更是禁止使用来降低设计的复杂性。递归一般来说也禁止使用。另一方面，HLS工具所能处理的范围非常广，包括DMA、流、片上内存访问等等，还能使用更为先进的优化技术如流水线、内存分块、位操作等等。 在Vivado HLS工具中定义了如下的规范： 不使用动态内存分配，如malloc()、free()、new、以及delete()。 减少使用指针来对指针进行操作。 不支持系统调用，如abort()、exit()、printf()。它们可以在测试例程中使用，但是在综合的时候会被忽略（移除）。 减少对标准库的使用。一般来说math.h是支持的，但是其他的标准库基本不支持。 减少使用函数指针以及C++类中的虚拟函数。（函数调用必须在编译时就由编译器完成） 不使用递归函数调用。 接口必须准确进行定义。 一个HLS工具的主要输出就是RTL级别的硬件设计。此外，HLS工具还可以输出测试的样例来验证设计。最后，工具还会提供对资源使用以及性能的大致估计。Vivado HLS会生成如下的输出： 可综合的Verilog和VHDL代码。 基于设计所生成的测试平台，可以进行RTL级别的仿真测试。 对性能以及资源使用的静态分析。 设计边界上的原数据可以使工具将设计更简单的组合成一个系统。 当一个RTL级别的设计生成之后，大多数的HLS工具则会进行标准的RTL设计流。在Xilinx Vivado的设计套件中，首先进行的是逻辑综合，将RTL级的设计转化成FPGA逻辑部件的连线表（netlist）。连线表（包括逻辑部件以及它们之间的连线）与目标设备中可用的资源相关联，这个过程称作布局以及布线（PAR，place and route）。对FPGA中资源最终的配置被写入一个比特流的文件中，可以上传到FPGA中实现。比特流文件实际上用每个位来代表对每个FPGA资源的配置，包括逻辑部、连线以及片上内存的配置。 FPGA 体系结构FPGA由一个可编程的逻辑块阵列以及存储部件组成，它们之间的连接使用可编程的内部连接。通常来说这些逻辑块会实现成查找表（lookup table，LUT）的形式——一种存储单元，输入的是地址信号而输出的是存储的内存单元。一个N位查找表可以以一个N位输入真值表的方式来表示。 上图a展示了一个2输入的LUT，也就是一个2-LUT。4个可配置的比特都可以用来编程以改变2-LUT的功能使其成为一个完整的可编程的2输入逻辑门。图b展示如何配置一个2-LUT使其称为一个与门，如上图所示，将Bit0~2配置为0，Bit3配置为1，就可以使得只有在两个输入都是1的时候，输出为1,。图c展示了一个简单的slice，里面含有稍微复杂的3-LUT结构，带有将输出存储到一个触发器（FF）当中的功能。需要配置的一共有9个比特，其中8个比特用于配置3-LUT，另外一个用来决定输出直接来自于3-LUT还是触发器。通常来说，一个slice会定义成一个包含有少量LUT以及触发器，并且在LUT以及触发器带有组合逻辑电路连接。 通过对配置比特的重新编程，FPGA板的功能可以很轻松的改变。大多数的FPGA使用的LUT带有4~6个输入的比特，这些LUT作为它们基本的计算部件。大型的FPGA可以拥有数以百万计的LUT。 触发器是FPGA中基本的存储单元。它们一般会和LUT一起组合成更为复杂的逻辑部件来称为一个可配置的逻辑块（configurable logic block，CLB）或者一个逻辑阵列块（logic array block，LAB）或者slice，这取决于你的设计工具或者供应商。一般来说，一个slice拥有LUT、触发器以及Mux（多路选择器）的数量并不会太多。一个slice也有可能使用更多更为复杂的逻辑功能器件。比如，slice中一般会带有一个全加器，这是因为在设计中全加器的使用太过于广泛，因此将所有的slice都配置有一个全加器带来更好的设计效益。 可重编程的内部互连也是FPGA另一个关键的要素。它提供了slice之间灵活的连接方式。slice的输入输出与连线通道相连。这个连线通道包含一组比特用来配置各个slice的输入输出之间是否相连。而通道本身则与开关盒相连。开关盒由很多传输晶体管充当的开关所组成，它的工作便是连接通道与通道。 上图提供了一个slice与连线通道以及开关盒之间的连线的例子。slice当中的每个输入输出都应该和连线通道的其中一条路线相连。可以将连线通道中的其中一条路线想象成一个单比特的连线。 一个开关盒为相邻的连接通道的连接路线提供了相连的矩阵（以类似矩阵的形式提供）。典型来说，一个FPGA可以看作是一种2D的逻辑表达形式，能够给设计者带来2D的抽象计算模型，这个通常称为岛状结构（island-style），其中slice表示一个“逻辑岛”（logic islands），slice之间通过连线通路以及开关盒来互相连接。 上图是一个2D的FPGA结构。可以看到，输入输出的块会提供一个外部的接口，可以连接到内存、微处理器、传感器等。在一些FPGA结构当中，IO会直接和片上的引脚进行连接。而在其他的一些FPGA当中会使用这些IO口和一些可编程的逻辑构造相连接通到片上的资源（一个微处理器总线或者缓存）。FPGA可编程的逻辑使用IO块来与外部设备进行沟通，可以是一个微控制器（一块片上的使用AXI总线接口的ARM处理器），可以是内存（片上的缓存或者片外的DRAM内存控制器），可以是传感器（AD）或者一个马达等。现今的FPGA片上都会集成有IO控制器，如内存控制器、AD转换、收发器等等。 现代的FPGA一般会在实现效率与灵活性之间做出平衡，并且，变得越来越多异构化，可能拥有大量不同的可编程的逻辑块以及一些预先配置好的体系架构的部件如寄存器组、自定义的数据路径以及高速的内部连接。下图是一个实现了DSP块的FPGA结构（专用于算术逻辑运算，特别是加法与乘法的预定义块）： 一个BRAM（block RAM）也是另一种预先配置好的资源的例子。BRAM是可配置的随机访问存储器模块，支持不同的内存形式以及接口。它们可以存储字节、半字、字或者双字。BRAM还可以把这些数据传输到连接到片上的总线接口（与可编程逻辑交流）或者处理器的总线（与片上处理器交流）上。一般来说，BRAM的作用是在片上资源之间传输数据（FPGA可编程逻辑以及片上微处理器）以及存放大的数据集。也可以在slice上对这些数据进行配置来存储（通过触发器），但是这样会带来额外的性能以及资源利用的开销。 典型的BRAM拥有大概32 Kbit的存储资源。它们可以配置成32K x 1bit，16K x 2bits，8K x 4bits等等。它们也可以相互级联来组成更大的存储。BRAM一般会和DSP48放置在一起。对于HLS来说，将BRAM看作是可配置的寄存器组更有益处。这些BRAM可以直接输出到自定义的数据路径（DSP48），并与片上的微处理器交流，传输数据给实现了可编程逻辑的自定义数据路径。下图是外部内存、BRAM以及触发器之间的比较： 随着片上的半导体变得越来越多，越来越多复杂，目前FPGA上集成的预先配置好的资源也越来越多复杂。现代的高端FPGA上可以集成多个微处理器核。一般来说，现在的小型的FPGA核也会继承一个微处理器核，这使得可以在片上运行操作系统，因此可以带来更多的功能，包括与外设交流、跑大型的软件程序如OpenCV，使用高级变成语言来设置系统以及快速运行相关的程序。微处理器一般会作为系统的控制器。它会控制从片外的内存、传感器到BRAM之间的数据流。并且微处理器还能协调各种IP核，包括HLS自带的IP核以及第三方的IP核与片上资源之间的关系。 FPGA设计过程FPGA设计通常由一个个大的部件或者IP核组成，如上图所示，一个假设的嵌入式FPGA设计过程，包括IO接口核、标准核以及特定应用加速器核的设计。注意到加速器核可以有流接口（加速器2）或者内存映射接口（加速器3），或者两者都有（加速器1）： 在靠近IO引脚的地方，会有少量的逻辑块实现一些关键的IO功能以及协议，比如内存控制块、视频接口核或者AD转换等。这个逻辑称为IO接口核（I/O interface core），一般来说会实现成结构化的RTL，经常带有额外的时序限制。时序限制的目的是阐明信号本身与信号变化规则之间的时序关系。IO接口核心在不同的FPGA架构上差别很大，一般由产商来提供。 除了IO引脚，FPGA设计还会包括标准核（standard cores），比如处理器核，片上内存以及内部连接开关。其他的标准和包括通用的、功能固定的处理部件，比如滤波器、FFT、编解码器等。这些核心的参数和接入方式在不同的设计中相差很大，但它们并不是在设计中真正造成差异的部件，相反他们是相对”水平的”技术部分，可以被插入到各类不同的应用领域。FPGA厂商同样也提供这些模块，但设计师其实很少情况下接触到它们。与IO接口核不同的是，标准核心主要是同步电路，它除了时钟时序限制之外不大有限制，因此这种核可以在不同的FPGA家族当中实现，因为它们的电路结构依然是高度优化过的。 近期FPGA的设计会包含自定义的，面向特定应用的加速器核（accelerator cores）。加速器核主要也是由时钟限制的同步电路组成的。HLS的关键也在于，如何快速而高校的设计出加速器核并很快的将其与系统相整合。 在继承上述设计的时候，有两种常用的设计方法。一种是将HLS生成的加速器核看成是其他核。用HLS创造出这种核之后把他们与IO接口核和标准核组合到一起，得到一个完整的设计。这种方法就称为以核为基础的设计方法（core-based design methodology），与不使用HLS进行FPGA设计开发相似。另外一种比较新的设计方法专注于标准的设计模板或者平台，这种方法下设计师先用IO接口核和标准核组合出一个样板，称为以平台为基础的设计方法（platform-based design methodology），可以使使用高级语言的程序员快速将多种不同的算法或者规则集成到提供单一平台或者shell的接口当中。同时它还能很容易的将加速器从一个平台移植到另一个平台。 设计优化性能特点计算的时间通常是一个很重要的设计性能质量的指标。当描述同步电路的时候，一般会使用时钟个数来作为性能衡量的指标。然而这对于有着不同时钟频率的架构来说是不适合的，这种情况在HLS中是很典型的。HLS可以根据时钟的不同来生成不同的架构。秒数则是一个比较恰当的指标，在HLS工具中（比如Vivado HLS）会回报程序执行的时钟周期个数以及时钟频率。 我们使用任务（task）来作为行为的一个基本单位。任务延迟（task latency）就是任务开始到任务完成中间的这段时间。任务间隔（task interval）则是任务开始到下一个任务开始之间的这段时间。所有任务的输入、输出以及计算都计算在任务延迟的时间之内，但是任务的开始不一定是读取输入，同样任务的结束不一定是写输出数据。在很多的设计当中，数据率（data rate）是一个关键的设计目标，依赖于任务间隔以及函数参数的多少。 上图展示了对一写假想应用的两种不同的设计方式。左边的图表示每个周期都执行新任务的结构设计，这是一种完全流水（fully-pipelined）的设计。右边的图表示的则是一个完全不一样的结构，系统每次读取四段输入，处理数据，然后再合成一个4段数据的输出。这种结构的任务延迟和任务间隔是一样的（13个周期），并且每一周期内只有一个任务在执行。这种架构和前者完全不同，前者在任何时刻都会有多个任务在同时执行。HLS中的流水线的思想和微处理器中的流水线很相似。然而，不同于使用简单的5级流水线，最后把结果写入寄存器组的方法，在Vivado HLS工具中，会构造一个只适用于特定板子，完成特定程序的电路。工具可以优化流水线级的数量，初始间隔（流水线连续读取两组数据之间的时间——与任务时间间隔相似），函数单元的数量以及类型和它们之间的互连情况（基于特定的程序以及设备）。 Vivado HLS工具通过计算输入与输出之间最大的寄存器数量来决定周期，因此0周期的情况是存在的（组合逻辑电路）。另一种方法是将输入和/或输入看作是一个寄存器来找到路径上最多的寄存器数量，这种方法一般会导致大量的周期产生。 面积与生产力（Throughput）的取舍考虑一个简单的通用硬件函数——有限脉冲响应（finite impulse response，FIR）滤波器。FIR会对输入做固定系数下的卷积，它可以被用作充当各式滤波器（高通，低通，带通），最简单的FIR可能就是一个移动平均滤波器。这在之后会详细提到，现在只考虑高层次抽象实现的问题。 上图是HLS中对一个功能或者一个任务的描述；上述的代码可以直接作为HLS工具的输入，然后会将其进行分析并产生对应功能的RTL级别的电路描述。可以把这个过程想象成一个编译过程，使用了一个类似于gcc一样的编译器，但是产生的不是汇编代码，HLS的“编译器”会产生RTL级的硬件电路描述。然而，理解HLS编译器的工作原理也是很重要的，因为像内存排布、流水线以及不同的IO接口对于HLS来说是很重要的，但是对于软件编译器来说并不重要。 产生电路的方式有很多种，这取决于你所使用的HLS工具是什么。一种可能的电路会顺序执行你的代码，就像一个简单的RISC处理器一样。此时，你可以想象成上述的代码编译成了能够在片上的微处理器运行的软件代码一样。下图是Xilinx Microblaze处理器对FIR滤波器生成的汇编代码： 假设每个周期发射一条指令，那么这段代码需要大约49个周期才能够输出滤波器的一个结果。显然，在这段代码中，对性能最大的一个障碍就是每个时钟周期能够执行多少条指令。HLS的其中一个特性就是，体系架构方面的取舍，可以脱离指令集架构的约束进行。在HLS的设计中，通常产生的体系架构会在每个时钟周期发射数百个甚至上千条RISC指令，而流水线的深度可以达到数百个周期。 Vivado HLS默认会生成一个优化的，大型的线性体系结构，在这其中，循环以及分支会转换成控制逻辑如控制寄存器以及相关的功能单元。概念上来说，这和RISC处理器的执行是很相似的，除了所要执行的程序转化成RTL级的有限状态机而不是从程序存储中获取指令执行。顺序化结构可以从大多数程序中生成，无需对原代码做太多的修改和优化，所以对HLS初学者非常的简单。但它同样存在一些缺陷。顺序化的结构很难解析码流，主要出于控制逻辑的复杂度。另外，控制逻辑负责规定任务延迟和任务间隔。顺序化结构的性能有时取决于处理的数据。 然而，Vivado HLS还可以生成高性能的流水线以及并行的架构。其中一个很重要的架构称为函数流水线（function pipeline）。一个函数流水线的架构将函数中的所有代码都看作是计算路径的一部分，以及一小部分的控制逻辑。代码中的循环和分支会转化为非条件的结构。结果是，这种架构相对来说比较简单，容易进行分析和理解，并且通常用于简单的、高数据率的设计，其中数据会连续的进行处理。函数流水线在大型的设计中比较有用。其比较明显的缺点是，不是所有的代码都能够有效的并行化处理。 Vivado HLS工具可以通过在函数体中声明 #pragma HLS pipeline 来产生一个函数流水线。这个指令需要一个参数来声明流水线的起始间隔，也就是函数流水线的任务间隔。下图展示了一种潜在的设计模式——一个“每周期一抽头”（one tap per clock）的架构，包括一个乘法器以及一个加法器来计算滤波器： 这个实现的任务间隔为4个时钟周期，任务延迟也是4个时钟周期。这种架构会在每4个时钟周期获取一个新的输入到滤波器中的样本并在输入4个周期后产生一个新的输出。下图是另外一种实现的方式，称为“每周期一样本”（one sample per clock）的体系结构，包括4个乘法器和3个加法器： 这个实现的任务间隔为1个时钟周期，任务延时也为1个时钟周期，表示在这个实现中每个时钟周期都会接受一个新的输入样本。 在实际操作当中，复杂的设计通常包含在顺序架构以及并行架构之间复杂的取舍。而在Vivado HLS当中，这种大量的取舍都会交由用户去完成。 处理速率的限制任何处理的架构当中任务间隔的大小都会有多种不同的基本限制。最重要的限制来源于递归（recurrences）或者设计中的反馈循环（feedback loops）。其他的一些关键的限制因素则来源于资源的限制。 递归（recurrences）的意思是，当前一个部件的计算依赖于现在在同一个部件上的计算结果。一个比较重要的概念是：递归是限制设计生产力的主要因素，即使是在流水线结构中也是如此。因此，对于HLS工具来说，分析算法中的递归并且生成正确的硬件结构是HLS工具的重要功能之一。类似的，理解算法并且尽量避免大量递归的实现也是HLS的重要功能。 递归在很多代码结构中都会出现，比如静态变量，顺序的循环。它存在于很多顺序化结构中，也有很多会随着改编成流水结构而消失。对于顺序化结构递归有时候不影响处理速率，但是在流水结构中是一个很不理想的状况。 另外一个对处理速率的限制就是资源本身的限制。资源限制的其中一种形式是设计边缘的跳线所致，因为一个同步的电路每根跳线每周期只能捕获并且传输一个比特的数据。结果就是，如果一个声明为int32_t f(int32_t x)的函数，实现在单一一个块中，且频率为100MHz，任务间隔为1个时钟周期，那么其最大的处理速率就是3.2GBits/s。另外一种资源限制来自于内存，因为绝大多数的内存每个时钟周期只支持有限次数的访问。还有一些其他的限制来自于用户的限制。 代码风格代码风格虽然是某种风格的表现，但是有时候它确实会限制HLS工具从代码中生成的架构。 举例来说，如下图所示，工具根据下面的代码会产生特别的架构。。这个情况下延迟线被展开，乘积的for循环都被用流水的方式实施，产出的结构会与“每个周期一个样本”的架构类似： 重构代码重构代码（Restructured code），在很多的情况下对于工具链来说都是很难理解的行为，需要对算法以及硬件结构有很深刻的了解。一般来说现成的算法原代码产出的结构比普通的CPU程序还低效，即使使用流水，展开等方法也没起到太大的作用。所以最好的方法还是自己写出一个等效但适合高层次综合的算法。 重构代码与其原先软件的实现区别很大——即使是高度优化过的也是如此。一些研究表明，重构代码是产生高效FPGA设计的必要步骤。因此，为了得到高效的硬件设计，用户需要在重构代码的过程中随时对底层的硬件结构映入脑海当中。]]></content>
      <categories>
        <category>HLS</category>
      </categories>
      <tags>
        <tag>Computer Architecture</tag>
        <tag>HLS</tag>
      </tags>
  </entry>
</search>
